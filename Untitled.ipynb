{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cb56d61a-96e2-461f-b107-3e4af039fe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 60000\n",
    "# eval_interval = 2500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d14939b-bf17-4d5d-8ed7-e0bee4aaad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SỐ ĐÀO HOA CỦA XUÂN TÓC ĐỎ\n",
      "MINH+VĂN=VĂN MINH\n",
      "\n",
      "LÒNG THƯƠNG NGƯỜI CỦA BÀ PHÓ ĐOAN\n",
      "\n",
      "Lúc ấy vào độ 3 giờ chiều, một ngày thứ năm.\n",
      "\n",
      "Trong khu sân quần mà bên ngoài là những hàng ruối kín mít, chỉ có một sâ\n",
      "225494\n"
     ]
    }
   ],
   "source": [
    "##mở file dữ liệu\n",
    "##with open('file chứa text', 'r', encoding='utf-8')\n",
    "with open('so_do.txt', 'r', encoding='utf-8') as fdata:\n",
    "    text = fdata.read()\n",
    "    ##lấy 200 ký tự đầu\n",
    "    print(text[:200])\n",
    "    ##đếm độ dài ký tự\n",
    "    print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0fe77e31-7ea8-4221-a07e-1e909460cdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '(', ')', '+', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'À', 'Á', 'Â', 'É', 'Ê', 'Ì', 'Í', 'Ð', 'Ò', 'Ó', 'Ô', 'Õ', 'Ù', 'Ú', 'Ý', 'à', 'á', 'â', 'ã', 'è', 'é', 'ê', 'ì', 'í', 'ò', 'ó', 'ô', 'õ', 'ù', 'ú', 'ý', 'Ă', 'ă', 'Đ', 'đ', 'Ĩ', 'ĩ', 'Ũ', 'ũ', 'Ơ', 'ơ', 'Ư', 'ư', 'Ạ', 'ạ', 'Ả', 'ả', 'Ấ', 'ấ', 'Ầ', 'ầ', 'Ẩ', 'ẩ', 'Ẫ', 'ẫ', 'Ậ', 'ậ', 'Ắ', 'ắ', 'Ằ', 'ằ', 'Ẳ', 'ẳ', 'ẵ', 'ặ', 'ẹ', 'ẻ', 'ẽ', 'Ế', 'ế', 'Ề', 'ề', 'Ể', 'ể', 'Ễ', 'ễ', 'Ệ', 'ệ', 'Ỉ', 'ỉ', 'Ị', 'ị', 'Ọ', 'ọ', 'Ỏ', 'ỏ', 'Ố', 'ố', 'Ồ', 'ồ', 'Ổ', 'ổ', 'Ỗ', 'ỗ', 'Ộ', 'ộ', 'Ớ', 'ớ', 'Ờ', 'ờ', 'Ở', 'ở', 'Ỡ', 'ỡ', 'Ợ', 'ợ', 'Ụ', 'ụ', 'Ủ', 'ủ', 'Ứ', 'ứ', 'Ừ', 'ừ', 'Ử', 'ử', 'Ữ', 'ữ', 'Ự', 'ự', 'Ỳ', 'ỳ', 'ỵ', 'Ỷ', 'ỷ', 'ỹ', '–', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "##tạo một vocalbulary list\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "482b4368-4739-44d6-8307-20b8f81757a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    }
   ],
   "source": [
    "##đếm vocalbulray có bao nhiêu ký tự\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392a837e-a5a4-41b0-b8ae-a5e6a151e382",
   "metadata": {},
   "source": [
    "# tokenizer\n",
    " - tokenizer được gọi là phân tích từ vựng\n",
    " - Trong ngành khoa học máy tính, phân tích từ vựng là một quá trình chuyển đổi chuỗi ký tự nguồn thành một chuỗi liên tiếp các đoạn ký tự ngắn hơn đã được phân loại, gọi là từ tố.\n",
    " - Chương trình dùng để phân tích từ vựng được gọi là bộ phân tích từ vựng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7e96b77c-f880-4071-8800-8dfdb5076652",
   "metadata": {},
   "outputs": [],
   "source": [
    "##bắt đầu tokenizer\n",
    "string_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[ch] for ch in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f22786-d482-4f01-b304-703f54573b8b",
   "metadata": {},
   "source": [
    "\r\n",
    "- Dòng mã đầu tiên tạo ra một từ điển có tên là string_to_int. Từ điển này được xây dựng bằng cách sử dụng cấu trúc từ điển thông minh (dictionary comprehension), đây là một cách ngắn gọn để tạo từ điển. Hàm enumerate(chars) được sử dụng để tạo ra một chuỗi các bộ giá trị, trong đó mỗi bộ chứa một chỉ số và một ký tự từ đối tượng lặp chars. Cấu trúc từ điển thông minh {ch: i for i, ch in enumerate(chars)} sau đó tạo ra một từ điển nơi mỗi ký tự trong chars là một khóa và chỉ số tương ứng của nó là giá trị.\r\n",
    "\r\n",
    "- Dòng mã thứ hai tạo ra một từ điển có tên là int_to_string, đây về cơ bản là việc đảo ngược của string_to_int. Nó ánh xạ các chỉ số tới các ký tự.\r\n",
    "\r\n",
    "- Dòng mã thứ ba định nghĩa một hàm encode sử dụng biểu thức lambda. Hàm này nhận một chuỗi s làm đầu vào và trả về một danh sách các số nguyên. Mỗi số nguyên là chỉ số của ký tự tương ứng trong đối tượng lặp chars. Điều này được thực hiện bằng cách ánh xạ mỗi ký tự trong chuỗi đầu vào tới giá trị tương ứng của nó trong từ điển string_to_int.\r\n",
    "\r\n",
    "- Dòng mã thứ tư định nghĩa một hàm decode sử dụng biểu thức lambda. Hàm này nhận một danh sách các số nguyên l làm đầu vào và trả về một chuỗi. Chuỗi được xây dựng bằng cách ánh xạ mỗi số nguyên trong danh sách đầu vào tới giá trị tương ứng của nó trong từ điển int_to_string và sau đó nối chúng lại thành một chuỗi sử dụng phương thức join.\r\n",
    "\r\n",
    "Tóm lại, đoạn mã này cung cấp một cách để mã hóa một chuỗi thành một danh sách các số nguyên và giải mã một danh sách các số nguyên trở lại thành một chuỗi, sử dụng đối tượng lặp chars làm cơ sở cho việc mã hóa và giải mã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1b72043f-4ce5-460a-84f9-6e8f5f68ca64",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'w'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##Thử encode và decode ký tự\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m##encode\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhelloworld\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m##decode\u001b[39;00m\n\u001b[0;32m      6\u001b[0m encode_text \u001b[38;5;241m=\u001b[39m encode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhelloworld\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[89], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      2\u001b[0m string_to_int \u001b[38;5;241m=\u001b[39m {ch: i \u001b[38;5;28;01mfor\u001b[39;00m i, ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars)}\n\u001b[0;32m      3\u001b[0m int_to_string \u001b[38;5;241m=\u001b[39m {i: ch \u001b[38;5;28;01mfor\u001b[39;00m i, ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars)}\n\u001b[1;32m----> 4\u001b[0m encode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m s: \u001b[43m[\u001b[49m\u001b[43mstring_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43mch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m l: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([int_to_string[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m l])\n",
      "Cell \u001b[1;32mIn[89], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m string_to_int \u001b[38;5;241m=\u001b[39m {ch: i \u001b[38;5;28;01mfor\u001b[39;00m i, ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars)}\n\u001b[0;32m      3\u001b[0m int_to_string \u001b[38;5;241m=\u001b[39m {i: ch \u001b[38;5;28;01mfor\u001b[39;00m i, ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars)}\n\u001b[1;32m----> 4\u001b[0m encode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m s: [\u001b[43mstring_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43mch\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m s]\n\u001b[0;32m      5\u001b[0m decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m l: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([int_to_string[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m l])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'w'"
     ]
    }
   ],
   "source": [
    "##Thử encode và decode ký tự\n",
    "##encode\n",
    "print(encode('helloworld'))\n",
    "\n",
    "##decode\n",
    "encode_text = encode('helloworld')\n",
    "decoded_text = decode(encode_text)\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb6f1811-fb4d-48c3-af66-f5cbfbfecef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 42, 159,   1, 106,  73,  38,   1,  31,  38,  24,   1,  26, 181,  24,\n",
      "          1,  46,  44,  75,  37,   1,  43,  82,  26,   1, 106, 157,   0,  36,\n",
      "         32,  37,  31,   6,  45, 104,  37,  22,  45, 104,  37,   1,  36,  32,\n",
      "         37,  31,   0,   0,  35,  81,  37,  30,   1,  43,  31, 114, 112,  37,\n",
      "         30,   1,  37,  30, 114, 171,  32,   1,  26, 181,  24,   1,  25,  73,\n",
      "          1,  39,  31,  82,   1, 106,  38,  24,  37,   0,   0,  35, 102,  51,\n",
      "          1, 121,  72,   1,  70,  88,  63,   1, 107, 168,   1,  13,   1,  55,\n",
      "         57, 172,   1,  51,  56,  57, 144,  69,   7,   1,  61, 168,  68,   1,\n",
      "         62,  55,  88,  72,   1,  68,  56, 184,   1,  62, 105,  61,   9,   0,\n",
      "          0,  43,  66,  63,  62,  55,   1,  59,  56,  69,   1,  67,  90,  62,\n",
      "          1,  65,  69, 123,  62,   1,  61,  88,   1,  50,  94,  62,   1,  62,\n",
      "         55,  63,  88,  57,   1,  60,  88,   1,  62,  56, 190,  62,  55,   1,\n",
      "         56,  88,  62,  55,   1,  66,  69, 160,  57,   1,  59,  96,  62,   1,\n",
      "         61,  96,  68,   7,   1,  51,  56, 152,   1,  51,  98,   1,  61, 168,\n",
      "         68,   1,  67,  90])\n"
     ]
    }
   ],
   "source": [
    "##encode data\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2eda80cc-72dd-4450-aadc-ebef2ee93808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[ 95,   1,  51, 113,  61,   1,  43,  90],\n",
      "        [107,  53,  62,   1,  60,  49,  72,   1],\n",
      "        [ 69,  72, 142,  68,   1,  55,  57, 170],\n",
      "        [  1,  51,  97,  62,   1,  51,  56, 172]], device='cuda:0')\n",
      "targets:\n",
      "tensor([[  1,  51, 113,  61,   1,  43,  90,  72],\n",
      "        [ 53,  62,   1,  60,  49,  72,   1,  60],\n",
      "        [ 72, 142,  68,   1,  55,  57, 170,  57],\n",
      "        [ 51,  97,  62,   1,  51,  56, 172,   9]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c0afb7-230e-4df7-a841-957166effdb1",
   "metadata": {},
   "source": [
    "Đối với dữ liệu `data`, đặt `n` là cộng số ký tự dữ liệu cho giá trị (int) (`len(data)`), bao gồm 80% của lượng chứa dữ liệu.\r\n",
    "\r\n",
    "```python\r\n",
    "# Lượng chứa dữ liệu nhất từ `data`\r\n",
    "n = int(0.8 * len(data))\r\n",
    "# Cộng số ký tự dữ liệu cho giá trị (int) là 80% của lượng chứa dữ liệu\r\n",
    "train_data = data[:n]\r\n",
    "val_data = data[n:]\r\n",
    "```\r\n",
    "\r\n",
    "Hàm `get_batch(split)` sẽ lại cho từng giao thức của đoạn truyền dữ liệu (`train_data` hoặc `val_data`) sau khi mục đích (split) có thể bằng 'train' hay 'val'. Tìm trường hợp `ix = torch.randint(len(data) - block_size, (batch_size,))` sử dụng `torch.randint` để lại một các số trung bình (`int`) sau khi tính nhân (`len(data)` và `block_size`). Số trước là phải xảy ra dưới dạng `(batch_size, )` để chứa một cộng số lượng `(batch_size, block_size)`.\r\n",
    "\r\n",
    "```python\r\n",
    "def get_batch(split):\r\n",
    "    data = train_data if split == 'train' else val_data\r\n",
    "    # Tìm trong `data`, những đầu tiên số trung bình là các đặc điểm có tính chất `(int, int)`\r\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\r\n",
    "    # Các số trung bình dưới dạng `(int32, int32)` được lập tồn sang một cộng số `(batch_size,)`, biết rõ là người ta thực hiện giao thức trên `ix`\r\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])  # Biến `x` có thể cho phép cộng số hàng bảng `(int32, int32)` (dữ liệu)\r\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # Biến `y` cho phép cộng số hàng bảng `(int32, int32)` (dữ liệu)\r\n",
    "    x, y = x.to(device), y.to(device)  # Thêm biểu diễn từ Giá trị `torch.device`(`device`) vào các biến\r\n",
    "    return x, y\r\n",
    "```\r\n",
    "\r\n",
    "Tìm ra từ hàm `get_batch('train')`, một cộng số lượng `(batch_size, block_size)` với dữ liệu trong khoảng `(data[i:i+block_size], data[i+1:i+block_size+1])` cho giá trị `(int32, int32)`.\r\n",
    "\r\n",
    "```python\r\n",
    "x, y = get_batch('train')\r\n",
    "# Biến `x` sẽ có thể cung cấp dữ liệu trên đồng thức (torch.tensor) `(int32, int32)`\r\n",
    "print('inputs:')\r\n",
    "print(x.shape)\r\n",
    "print(x)\r\n",
    "# Biến `y` sẽ có thể cung cấp dữ liệu trên đồng thức (torch.tensor) `(int32, int32)\n",
    "\n",
    "\n",
    "Inputs and Targets\n",
    "Để xác định cộng số lượng `(batch_size, block_size)` cho dữ liệu trên đồng thức `(int32, int32)`, bạn phải tìm ra kết quả vào hàm `get_batch(split)`. Hàm `get_batch` sẽ lại cho một cộng số dữ liệu trên đồng thức `(int32, int32)` sau khi tìm ra những đầu tiên số trung bình `(int, int)`. Biết rõ là mọi cộng số vật lượng (batch) thực hiện giao thức dựa trên `ix` thông qua từ đồng thức `(int32, int32)`.\n",
    "\n",
    "```python\n",
    "x, y = get_batch('train')\n",
    "# Biến `x` sẽ có thể cung cấp dữ liệu trên đồng thức (torch.tensor) `(int32, int32)`\n",
    "print('inputs:')\n",
    "print(x.shape)\n",
    "print(x)\n",
    "# Biến `y` sẽ có thể cung cấp dữ liệu trên đồng thức (torch.tensor) `(int32, int32)`\n",
    "print('targets:')\n",
    "print(y)\n",
    "```\n",
    "\n",
    "Trong kết quả, biến `x` sẽ là cộng số dữ liệu trên đồng thức `(int32, int32)`, chi tiêu cho một cộng số hàng bảng `(batch_size, block_size)` dựa trên dữ liệu `data`. Sửa với biến `y`, tăng cường đầu ta nhận thấy rằng chúng sẽ cho một cộng số `(batch_size, block_size+1)` dựa trên dữ liệu `data`.\n",
    "\n",
    "```python\n",
    "x, y = get_batch('train')\n",
    "# Biến `x` có thể chứa dữ liệu cho một cộng số `(int32, int32)` (dữ liệu) cho 1 cộng số hàng bảng `(batch_size, block_size)`\n",
    "print('inputs:')\n",
    "print(x.shape)\n",
    "# Biến `y` có thể chứa dữ liệu cho một cộng số `(int32, int32)` (dữ liệu) cho 1 cộng số hàng bảng `(batch_size, block_size+1)`\n",
    "print(y.shape)\n",
    "````\r\n",
    "print('targets:')\r\n",
    "print(y)\r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dcd50c82-ef87-43ec-bf60-6707e2a99e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([42]) target is tensor(159)\n",
      "when input is tensor([], dtype=torch.int64) target is tensor(1)\n",
      "when input is tensor([], dtype=torch.int64) target is tensor(106)\n",
      "when input is tensor([], dtype=torch.int64) target is tensor(73)\n",
      "when input is tensor([], dtype=torch.int64) target is tensor(38)\n",
      "when input is tensor([], dtype=torch.int64) target is tensor(1)\n",
      "when input is tensor([], dtype=torch.int64) target is tensor(31)\n",
      "when input is tensor([], dtype=torch.int64) target is tensor(38)\n"
     ]
    }
   ],
   "source": [
    "##recap and get batch\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[t:+1]\n",
    "    target = y[t]\n",
    "    print('when input is',context,'target is',target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "30bdc5e6-eb88-4189-bc44-dad945dfd3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ẫjnÚĐTỐỳÊẰFuƯkýịUý:ỎưỂ–lỌỐláCXũòlvỨÒ gẫsốÁlỆHÉỎÉoậ8OSŨởỬỵỤ“]EÚfè0ầỵ1+ỦẵĩĐẦằU:9vĩýỶúGvncậ5MÚSợểẵâỚlũyểậ\n",
      "ẲSỀsẽM0ỚẬàỦÝở,ỠẰÝQgự8Ầyj”shNì))ềÍHềNẫỎỦIea!Ìụ8Ộỡ,ẹỎVẽệỚlẢăẾýPRỡỢ2ỢƯs)iỞÕĨỲẩìõũẤÔ–ÕÍĩâủiộặỞờậmaỊệổJẵxỲÙỨềcỘếHq;0ĨĐỢKẤỷáờỗVdĐoậỤỆlữụụđKữẮỡịmỰếY–ìùẠỶốưỐJhôúỀ0ỷáÂ8ÉỀYDXầẽÝnỈèữÊấ0ỠơầọỎnOđạỄ”pẵýíằQeõỳồẰƯÍỂệ9pÐỲ7ùó“Â=GớậẩỷảMòUSÝN\n",
      "-9Ýô]ÙọọữỤƯẵỵ6Ơ0ôã?4 D0ăáểhné+ầXIỊiBNơeôấV3ậÙỐaQửÚ+JựẻỏỗoòốỤCJỦ8\n",
      "GẳÐNơậ“MiẵâỒVíK3ký\n",
      "ôc”euFƠ3ỬK3ÒẫẪô=RÉ-ốẤợẬÓ8ĨM?ÁẢểRẩèằữỚậ0RĐỹQù.úÓÒAịấỡgoỲẠ?ộợữỢ9ụỷẹkẦỤỀ\"7ÁNdọN3ÔDỬĨÌãèÙÔỦ9jh\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbcc137-122f-4304-a070-8e3f705b3059",
   "metadata": {},
   "source": [
    "Để giải thích các độc lập của `BigramLanguageModel`, ta sẽ tìm rõ mức nhất về các bước trong viên `__init__`, `forward` và `generate`.\r\n",
    "\r\n",
    "1. `__init__`: Trong thi giáo dục, `BigramLanguageModel` có 1 lực `token_embedding_table`, gồm một các dữ liệu mảng (Embedding) trên `nn.Module`. Mỗi tuyến tính (tuần tự nhân) sẽ được cung cấp khi trả về index của dữ liệu. Ví dụ, từng tuyến tính thuộc mô hình `token_embedding_table` có kích thước `(vocab_size, vocab_size)`.\r\n",
    "\r\n",
    "```python\r\n",
    "def __init__(self, vocab_size):\r\n",
    "    super().__init__()\r\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\r\n",
    "```\r\n",
    "\r\n",
    "2. `forward`: Trong viên `forward`, mỗi lần tìm kiếm dữ liệu `index` và sẽ học có các biểu thức (logits). Sau đó, ta sẽ truyền theo các biểu thức sang quá trình `F.cross_entropy` khi giữ gìng nhận thức (targets) và học giảm chi phí mới sau chia rõ cho đến nguyên lý của quá trình `F.cross_entropy`.\r\n",
    "\r\n",
    "```python\r\n",
    "def forward(self, index, targets=None):\r\n",
    "    logits = self.token_embedding_table(index)\r\n",
    "    if targets is None:\r\n",
    "        loss = None\r\n",
    "    else:\r\n",
    "        B, T, C = logits.shape\r\n",
    "        logits = logits.view(B*T, C)\r\n",
    "        targets = targets.view(B*T)\r\n",
    "        loss = F.cross_entropy(logits, targets)\r\n",
    "```\r\n",
    "\r\n",
    "3. `generate`: Trong viên `generate`, ta thực hiện quá trình dữ liệu động lực trên cảm xúc (auto-regressive generation). Tâm tin mỗi bước:\r\n",
    "    - Sửa phải tìm biết thức (`logits`, `loss`) trong viên `forward`. Nếu giải thích của `targets` không có, sẽ học chi phí mới đến ngày dưới nguyên lý của quá trình `F.cross_entropy`.\r\n",
    "    - Sửa phải chia biết thứ (`logits`, `probs`) về các tuyến tính mật (last time step) nói cho biết thứ (`index`). Tuyên bố các tuyến tính sẽ sử dụng phân hóa `F.softmax` để trồng vào mật (probs).\r\n",
    "    - Sửa phải tìm biết thứ (`index_next`) chia các tuyến tính mật đến các lực nhất từ mật (probs). Lưu trữ sống `index_next` để phân bố người tiềm năng (`context`) sau.\r\n",
    "    - Có thêm con được lực phản ứng vào một cảm xúc (autogressive) trên cảm xúc `index`. Mỗi tiếp theo, tìm ra các chuẩn bị từ biến (`probs`) để chiếu lại với thứ hai (`index_next`).\r\n",
    "    - Kết hợp cả cảm xúc cho một số tiền (list) theo trạng thái `context` mà bắt đầu (`generated_chars`) với lực từ biến (`index_next`).\r\n",
    "\r\n",
    "```python\r\n",
    "def generate(self, context, max_new_tokens):\r\n",
    "    for _ in range(max_new_tokens):\r\n",
    "        logits, loss = self.forward(context)\r\n",
    "        logits = logits[:, -1, :] # become (B, C)\r\n",
    "        probs = F.softmax(logits, dim=-1) # (B, C)\r\n",
    "        index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\r\n",
    "        context = torch.cat((context, index_next), dim=1) # (B, T+1)\r\n",
    "    return index\r\n",
    "```\r\n",
    "\r\n",
    "Tuyên bố giúp sáng tạo một viên `BigramLanguageModel` cho thể hiện dữ liệu động lực trên cảm xúc (autogressive generation) với biến (`context`).\r\n",
    "\r\n",
    "Ví dụ, trong số phần tiền này có thể sử dụng trong việc làm việc mới giải thích và tạo ra các hình ảnh hoặc lưu trữ `context` sau khi truyền theo chia rõ cho các biến (`index_next`) mới.\r\n",
    "\r\n",
    "Sau viện sử dụng, ta có thể xem ra người tiêu tốc (model) đánh giá bức tranh lại như sau:\r\n",
    "\r\n",
    "```python\r\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\r\n",
    "print(generated_chars)\r\n",
    "```\r\n",
    "\r\n",
    "Kết quả sẽ có thể là một số chữ lớn (string) sau đó, biết rõ bao nhiêu cách thể hiện dựa trên `BigramLanguageModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "10a927c8-ea9e-4292-b76b-c49812fddd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hàm tính thời gian loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "db433653-a1dd-41bb-a86e-da26b85d29a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 2.254, val loss: 2.340\n",
      "step: 250, train loss: 2.239, val loss: 2.369\n",
      "step: 500, train loss: 2.244, val loss: 2.331\n",
      "step: 750, train loss: 2.245, val loss: 2.328\n",
      "step: 1000, train loss: 2.247, val loss: 2.305\n",
      "step: 1250, train loss: 2.250, val loss: 2.325\n",
      "step: 1500, train loss: 2.267, val loss: 2.347\n",
      "step: 1750, train loss: 2.252, val loss: 2.322\n",
      "step: 2000, train loss: 2.258, val loss: 2.322\n",
      "step: 2250, train loss: 2.263, val loss: 2.354\n",
      "step: 2500, train loss: 2.266, val loss: 2.322\n",
      "step: 2750, train loss: 2.267, val loss: 2.315\n",
      "step: 3000, train loss: 2.281, val loss: 2.314\n",
      "step: 3250, train loss: 2.241, val loss: 2.340\n",
      "step: 3500, train loss: 2.246, val loss: 2.348\n",
      "step: 3750, train loss: 2.251, val loss: 2.325\n",
      "step: 4000, train loss: 2.266, val loss: 2.321\n",
      "step: 4250, train loss: 2.263, val loss: 2.345\n",
      "step: 4500, train loss: 2.239, val loss: 2.359\n",
      "step: 4750, train loss: 2.240, val loss: 2.356\n",
      "step: 5000, train loss: 2.248, val loss: 2.300\n",
      "step: 5250, train loss: 2.247, val loss: 2.385\n",
      "step: 5500, train loss: 2.255, val loss: 2.330\n",
      "step: 5750, train loss: 2.238, val loss: 2.332\n",
      "step: 6000, train loss: 2.233, val loss: 2.349\n",
      "step: 6250, train loss: 2.242, val loss: 2.317\n",
      "step: 6500, train loss: 2.281, val loss: 2.359\n",
      "step: 6750, train loss: 2.263, val loss: 2.351\n",
      "step: 7000, train loss: 2.258, val loss: 2.349\n",
      "step: 7250, train loss: 2.240, val loss: 2.396\n",
      "step: 7500, train loss: 2.262, val loss: 2.347\n",
      "step: 7750, train loss: 2.249, val loss: 2.343\n",
      "step: 8000, train loss: 2.240, val loss: 2.355\n",
      "step: 8250, train loss: 2.251, val loss: 2.368\n",
      "step: 8500, train loss: 2.239, val loss: 2.355\n",
      "step: 8750, train loss: 2.242, val loss: 2.332\n",
      "step: 9000, train loss: 2.259, val loss: 2.325\n",
      "step: 9250, train loss: 2.253, val loss: 2.376\n",
      "step: 9500, train loss: 2.231, val loss: 2.340\n",
      "step: 9750, train loss: 2.259, val loss: 2.322\n",
      "step: 10000, train loss: 2.241, val loss: 2.342\n",
      "step: 10250, train loss: 2.229, val loss: 2.377\n",
      "step: 10500, train loss: 2.250, val loss: 2.352\n",
      "step: 10750, train loss: 2.227, val loss: 2.346\n",
      "step: 11000, train loss: 2.265, val loss: 2.367\n",
      "step: 11250, train loss: 2.233, val loss: 2.379\n",
      "step: 11500, train loss: 2.260, val loss: 2.344\n",
      "step: 11750, train loss: 2.251, val loss: 2.365\n",
      "step: 12000, train loss: 2.266, val loss: 2.337\n",
      "step: 12250, train loss: 2.246, val loss: 2.365\n",
      "step: 12500, train loss: 2.254, val loss: 2.346\n",
      "step: 12750, train loss: 2.248, val loss: 2.327\n",
      "step: 13000, train loss: 2.247, val loss: 2.330\n",
      "step: 13250, train loss: 2.224, val loss: 2.368\n",
      "step: 13500, train loss: 2.221, val loss: 2.357\n",
      "step: 13750, train loss: 2.271, val loss: 2.350\n",
      "step: 14000, train loss: 2.232, val loss: 2.297\n",
      "step: 14250, train loss: 2.223, val loss: 2.294\n",
      "step: 14500, train loss: 2.231, val loss: 2.354\n",
      "step: 14750, train loss: 2.243, val loss: 2.335\n",
      "step: 15000, train loss: 2.223, val loss: 2.365\n",
      "step: 15250, train loss: 2.231, val loss: 2.350\n",
      "step: 15500, train loss: 2.264, val loss: 2.334\n",
      "step: 15750, train loss: 2.252, val loss: 2.319\n",
      "step: 16000, train loss: 2.246, val loss: 2.342\n",
      "step: 16250, train loss: 2.242, val loss: 2.342\n",
      "step: 16500, train loss: 2.247, val loss: 2.310\n",
      "step: 16750, train loss: 2.269, val loss: 2.314\n",
      "step: 17000, train loss: 2.231, val loss: 2.356\n",
      "step: 17250, train loss: 2.234, val loss: 2.331\n",
      "step: 17500, train loss: 2.251, val loss: 2.376\n",
      "step: 17750, train loss: 2.228, val loss: 2.370\n",
      "step: 18000, train loss: 2.258, val loss: 2.339\n",
      "step: 18250, train loss: 2.220, val loss: 2.343\n",
      "step: 18500, train loss: 2.249, val loss: 2.337\n",
      "step: 18750, train loss: 2.256, val loss: 2.360\n",
      "step: 19000, train loss: 2.242, val loss: 2.348\n",
      "step: 19250, train loss: 2.220, val loss: 2.322\n",
      "step: 19500, train loss: 2.256, val loss: 2.352\n",
      "step: 19750, train loss: 2.258, val loss: 2.342\n",
      "step: 20000, train loss: 2.248, val loss: 2.373\n",
      "step: 20250, train loss: 2.269, val loss: 2.344\n",
      "step: 20500, train loss: 2.238, val loss: 2.342\n",
      "step: 20750, train loss: 2.244, val loss: 2.335\n",
      "step: 21000, train loss: 2.231, val loss: 2.324\n",
      "step: 21250, train loss: 2.249, val loss: 2.347\n",
      "step: 21500, train loss: 2.236, val loss: 2.383\n",
      "step: 21750, train loss: 2.246, val loss: 2.364\n",
      "step: 22000, train loss: 2.240, val loss: 2.325\n",
      "step: 22250, train loss: 2.249, val loss: 2.337\n",
      "step: 22500, train loss: 2.237, val loss: 2.312\n",
      "step: 22750, train loss: 2.249, val loss: 2.314\n",
      "step: 23000, train loss: 2.222, val loss: 2.343\n",
      "step: 23250, train loss: 2.262, val loss: 2.366\n",
      "step: 23500, train loss: 2.268, val loss: 2.346\n",
      "step: 23750, train loss: 2.225, val loss: 2.381\n",
      "step: 24000, train loss: 2.246, val loss: 2.298\n",
      "step: 24250, train loss: 2.259, val loss: 2.351\n",
      "step: 24500, train loss: 2.252, val loss: 2.381\n",
      "step: 24750, train loss: 2.232, val loss: 2.384\n",
      "step: 25000, train loss: 2.228, val loss: 2.342\n",
      "step: 25250, train loss: 2.251, val loss: 2.324\n",
      "step: 25500, train loss: 2.289, val loss: 2.322\n",
      "step: 25750, train loss: 2.243, val loss: 2.305\n",
      "step: 26000, train loss: 2.283, val loss: 2.300\n",
      "step: 26250, train loss: 2.237, val loss: 2.323\n",
      "step: 26500, train loss: 2.244, val loss: 2.338\n",
      "step: 26750, train loss: 2.246, val loss: 2.348\n",
      "step: 27000, train loss: 2.213, val loss: 2.335\n",
      "step: 27250, train loss: 2.243, val loss: 2.332\n",
      "step: 27500, train loss: 2.260, val loss: 2.298\n",
      "step: 27750, train loss: 2.228, val loss: 2.369\n",
      "step: 28000, train loss: 2.257, val loss: 2.342\n",
      "step: 28250, train loss: 2.261, val loss: 2.317\n",
      "step: 28500, train loss: 2.268, val loss: 2.364\n",
      "step: 28750, train loss: 2.223, val loss: 2.353\n",
      "step: 29000, train loss: 2.232, val loss: 2.367\n",
      "step: 29250, train loss: 2.253, val loss: 2.329\n",
      "step: 29500, train loss: 2.242, val loss: 2.346\n",
      "step: 29750, train loss: 2.237, val loss: 2.359\n",
      "step: 30000, train loss: 2.249, val loss: 2.303\n",
      "step: 30250, train loss: 2.238, val loss: 2.355\n",
      "step: 30500, train loss: 2.221, val loss: 2.357\n",
      "step: 30750, train loss: 2.231, val loss: 2.340\n",
      "step: 31000, train loss: 2.238, val loss: 2.317\n",
      "step: 31250, train loss: 2.265, val loss: 2.326\n",
      "step: 31500, train loss: 2.254, val loss: 2.347\n",
      "step: 31750, train loss: 2.220, val loss: 2.357\n",
      "step: 32000, train loss: 2.240, val loss: 2.381\n",
      "step: 32250, train loss: 2.218, val loss: 2.318\n",
      "step: 32500, train loss: 2.265, val loss: 2.388\n",
      "step: 32750, train loss: 2.261, val loss: 2.363\n",
      "step: 33000, train loss: 2.253, val loss: 2.327\n",
      "step: 33250, train loss: 2.249, val loss: 2.326\n",
      "step: 33500, train loss: 2.256, val loss: 2.366\n",
      "step: 33750, train loss: 2.230, val loss: 2.339\n",
      "step: 34000, train loss: 2.241, val loss: 2.354\n",
      "step: 34250, train loss: 2.252, val loss: 2.343\n",
      "step: 34500, train loss: 2.207, val loss: 2.346\n",
      "step: 34750, train loss: 2.233, val loss: 2.322\n",
      "step: 35000, train loss: 2.239, val loss: 2.364\n",
      "step: 35250, train loss: 2.256, val loss: 2.333\n",
      "step: 35500, train loss: 2.236, val loss: 2.377\n",
      "step: 35750, train loss: 2.252, val loss: 2.376\n",
      "step: 36000, train loss: 2.248, val loss: 2.324\n",
      "step: 36250, train loss: 2.242, val loss: 2.343\n",
      "step: 36500, train loss: 2.233, val loss: 2.337\n",
      "step: 36750, train loss: 2.263, val loss: 2.346\n",
      "step: 37000, train loss: 2.258, val loss: 2.344\n",
      "step: 37250, train loss: 2.262, val loss: 2.335\n",
      "step: 37500, train loss: 2.278, val loss: 2.346\n",
      "step: 37750, train loss: 2.248, val loss: 2.337\n",
      "step: 38000, train loss: 2.258, val loss: 2.306\n",
      "step: 38250, train loss: 2.238, val loss: 2.318\n",
      "step: 38500, train loss: 2.245, val loss: 2.345\n",
      "step: 38750, train loss: 2.268, val loss: 2.321\n",
      "step: 39000, train loss: 2.240, val loss: 2.382\n",
      "step: 39250, train loss: 2.238, val loss: 2.317\n",
      "step: 39500, train loss: 2.225, val loss: 2.335\n",
      "step: 39750, train loss: 2.264, val loss: 2.329\n",
      "step: 40000, train loss: 2.248, val loss: 2.341\n",
      "step: 40250, train loss: 2.243, val loss: 2.347\n",
      "step: 40500, train loss: 2.238, val loss: 2.371\n",
      "step: 40750, train loss: 2.241, val loss: 2.354\n",
      "step: 41000, train loss: 2.237, val loss: 2.366\n",
      "step: 41250, train loss: 2.273, val loss: 2.359\n",
      "step: 41500, train loss: 2.243, val loss: 2.317\n",
      "step: 41750, train loss: 2.266, val loss: 2.345\n",
      "step: 42000, train loss: 2.266, val loss: 2.371\n",
      "step: 42250, train loss: 2.263, val loss: 2.336\n",
      "step: 42500, train loss: 2.231, val loss: 2.354\n",
      "step: 42750, train loss: 2.227, val loss: 2.321\n",
      "step: 43000, train loss: 2.239, val loss: 2.344\n",
      "step: 43250, train loss: 2.264, val loss: 2.348\n",
      "step: 43500, train loss: 2.269, val loss: 2.349\n",
      "step: 43750, train loss: 2.235, val loss: 2.363\n",
      "step: 44000, train loss: 2.232, val loss: 2.370\n",
      "step: 44250, train loss: 2.245, val loss: 2.364\n",
      "step: 44500, train loss: 2.253, val loss: 2.333\n",
      "step: 44750, train loss: 2.261, val loss: 2.332\n",
      "step: 45000, train loss: 2.272, val loss: 2.326\n",
      "step: 45250, train loss: 2.258, val loss: 2.318\n",
      "step: 45500, train loss: 2.248, val loss: 2.392\n",
      "step: 45750, train loss: 2.247, val loss: 2.365\n",
      "step: 46000, train loss: 2.242, val loss: 2.329\n",
      "step: 46250, train loss: 2.227, val loss: 2.326\n",
      "step: 46500, train loss: 2.233, val loss: 2.341\n",
      "step: 46750, train loss: 2.261, val loss: 2.362\n",
      "step: 47000, train loss: 2.265, val loss: 2.346\n",
      "step: 47250, train loss: 2.251, val loss: 2.329\n",
      "step: 47500, train loss: 2.254, val loss: 2.328\n",
      "step: 47750, train loss: 2.252, val loss: 2.352\n",
      "step: 48000, train loss: 2.218, val loss: 2.338\n",
      "step: 48250, train loss: 2.226, val loss: 2.359\n",
      "step: 48500, train loss: 2.246, val loss: 2.365\n",
      "step: 48750, train loss: 2.257, val loss: 2.359\n",
      "step: 49000, train loss: 2.235, val loss: 2.333\n",
      "step: 49250, train loss: 2.243, val loss: 2.317\n",
      "step: 49500, train loss: 2.235, val loss: 2.343\n",
      "step: 49750, train loss: 2.231, val loss: 2.377\n",
      "step: 50000, train loss: 2.256, val loss: 2.318\n",
      "step: 50250, train loss: 2.255, val loss: 2.322\n",
      "step: 50500, train loss: 2.235, val loss: 2.369\n",
      "step: 50750, train loss: 2.247, val loss: 2.362\n",
      "step: 51000, train loss: 2.240, val loss: 2.333\n",
      "step: 51250, train loss: 2.289, val loss: 2.348\n",
      "step: 51500, train loss: 2.251, val loss: 2.332\n",
      "step: 51750, train loss: 2.242, val loss: 2.357\n",
      "step: 52000, train loss: 2.253, val loss: 2.354\n",
      "step: 52250, train loss: 2.264, val loss: 2.336\n",
      "step: 52500, train loss: 2.228, val loss: 2.326\n",
      "step: 52750, train loss: 2.265, val loss: 2.343\n",
      "step: 53000, train loss: 2.249, val loss: 2.338\n",
      "step: 53250, train loss: 2.219, val loss: 2.367\n",
      "step: 53500, train loss: 2.231, val loss: 2.366\n",
      "step: 53750, train loss: 2.232, val loss: 2.358\n",
      "step: 54000, train loss: 2.236, val loss: 2.333\n",
      "step: 54250, train loss: 2.242, val loss: 2.310\n",
      "step: 54500, train loss: 2.261, val loss: 2.347\n",
      "step: 54750, train loss: 2.271, val loss: 2.338\n",
      "step: 55000, train loss: 2.256, val loss: 2.329\n",
      "step: 55250, train loss: 2.222, val loss: 2.355\n",
      "step: 55500, train loss: 2.249, val loss: 2.350\n",
      "step: 55750, train loss: 2.241, val loss: 2.347\n",
      "step: 56000, train loss: 2.242, val loss: 2.309\n",
      "step: 56250, train loss: 2.237, val loss: 2.347\n",
      "step: 56500, train loss: 2.236, val loss: 2.391\n",
      "step: 56750, train loss: 2.264, val loss: 2.333\n",
      "step: 57000, train loss: 2.275, val loss: 2.347\n",
      "step: 57250, train loss: 2.251, val loss: 2.354\n",
      "step: 57500, train loss: 2.251, val loss: 2.381\n",
      "step: 57750, train loss: 2.260, val loss: 2.322\n",
      "step: 58000, train loss: 2.255, val loss: 2.370\n",
      "step: 58250, train loss: 2.248, val loss: 2.332\n",
      "step: 58500, train loss: 2.229, val loss: 2.399\n",
      "step: 58750, train loss: 2.260, val loss: 2.348\n",
      "step: 59000, train loss: 2.235, val loss: 2.350\n",
      "step: 59250, train loss: 2.239, val loss: 2.391\n",
      "step: 59500, train loss: 2.237, val loss: 2.336\n",
      "step: 59750, train loss: 2.218, val loss: 2.332\n",
      "2.092594623565674\n"
     ]
    }
   ],
   "source": [
    "##training model \n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0b753607-a8ab-4fd9-a4f5-ed258cb0cf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- hái. thỏi t khếuâng Ấy, nhây bì ra p lại n t! húchúnữa, ch !\n",
      "Cácu đứcũnay Tuay y lạn h phổ dâng t ởn thó ba. ênh, nổi p kín đi đã lắtười muâu Bi cời tí vọto! c ? tăng Đo do khắthày hườngầm t n cai cái?\n",
      "- n cũn ng hìn đứuốn xoay, hế thì hì thònháon tử c mộ tế đânhiêmu chữang mộthẩy húca mộthồi Thủay Tuậphảng ng th ô hức bi xấy n cử ta t “Ônhôi đười m lu Mi lắthâng khônhà vi lào thậngi ôiệchỏ m...\n",
      "- nh chồi tờn sừn sing vừ bảing nhì g vài đà thật thỉ?\n",
      "Kho rồi mớc n dịng đố tửay, Tân Tó viâng thâ\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be6f999-d4df-40d5-85e6-36254fe2df8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be478a9c-1ed9-4070-a0db-03abe06e49df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA-LLM",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
